{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# RAG with Qdrant: Building a Textbook Q&A System\n",
    "\n",
    "This notebook demonstrates a complete **Retrieval-Augmented Generation (RAG)** pipeline using Qdrant.\n",
    "\n",
    "**What you'll learn:**\n",
    "1. Loading a textbook from Azure Blob Storage\n",
    "2. Chunking documents into 700-token pieces\n",
    "3. Creating embeddings and storing them in Qdrant\n",
    "4. How user prompts become embedded queries\n",
    "5. How vector similarity lookup works\n",
    "6. How context is retrieved and formatted for an LLM\n",
    "7. Building your own RAG system\n",
    "\n",
    "**Prerequisites:** Basic understanding of embeddings and vector databases (see Part 1-3 notebooks)\n",
    "\n",
    "**Note:** This notebook demonstrates the RAG retrieval pipeline. The actual LLM call is shown conceptually - you can integrate with OpenAI, Azure OpenAI, or other LLMs."
   ],
   "id": "a78d41d895f73b4f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 1: Setup and Installation\n",
    "\n",
    "First, let's install all required packages."
   ],
   "id": "411907232976a791"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Install required packages\n",
    "!pip install qdrant-client sentence-transformers azure-storage-blob tiktoken -q\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ],
   "id": "bf4cf8e1edd656e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 2: Loading the Textbook from Azure\n",
    "\n",
    "We'll load a textbook from Azure Blob Storage using a shared access key.\n",
    "\n",
    "**In class, you'll receive:**\n",
    "- Azure Storage Account URL\n",
    "- Container name\n",
    "- Blob name (textbook file)\n",
    "- SAS token for access"
   ],
   "id": "c359040d98c74eac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "import os\n",
    "\n",
    "# ‚ö†Ô∏è REPLACE THESE WITH VALUES PROVIDED IN CLASS\n",
    "AZURE_STORAGE_URL = \"https://your-storage-account.blob.core.windows.net\"\n",
    "CONTAINER_NAME = \"textbooks\"\n",
    "BLOB_NAME = \"sample-textbook.txt\"\n",
    "SAS_TOKEN = \"?sv=2021-06-08&ss=b&srt=sco&sp=r&se=...\"  # Provided in class\n",
    "\n",
    "# For this demo, we'll use a sample text if Azure credentials aren't available\n",
    "USE_AZURE = False  # Set to True when you have credentials\n",
    "\n",
    "if USE_AZURE:\n",
    "    # Connect to Azure Blob Storage\n",
    "    blob_service_client = BlobServiceClient(account_url=AZURE_STORAGE_URL, credential=SAS_TOKEN)\n",
    "    blob_client = blob_service_client.get_blob_client(container=CONTAINER_NAME, blob=BLOB_NAME)\n",
    "    \n",
    "    # Download the textbook\n",
    "    print(f\"üì• Downloading textbook from Azure...\")\n",
    "    textbook_content = blob_client.download_blob().readall().decode('utf-8')\n",
    "    print(f\"‚úÖ Downloaded {len(textbook_content)} characters\")\n",
    "else:\n",
    "    # Demo textbook content about machine learning\n",
    "    print(\"üìö Using demo textbook content...\")\n",
    "    textbook_content = \"\"\"\n",
    "Chapter 1: Introduction to Machine Learning\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables computers to learn from data without being explicitly programmed. The field has revolutionized how we approach problem-solving in computer science.\n",
    "\n",
    "Section 1.1: What is Machine Learning?\n",
    "\n",
    "Machine learning algorithms build mathematical models based on sample data, known as training data, to make predictions or decisions. Unlike traditional programming where rules are explicitly coded, machine learning systems discover patterns in data.\n",
    "\n",
    "There are three main types of machine learning: supervised learning, unsupervised learning, and reinforcement learning. Each type serves different purposes and uses different approaches to learn from data.\n",
    "\n",
    "Section 1.2: Supervised Learning\n",
    "\n",
    "Supervised learning is the most common type of machine learning. In supervised learning, the algorithm learns from labeled training data. Each training example consists of an input and the desired output. The algorithm learns to map inputs to outputs.\n",
    "\n",
    "Common supervised learning tasks include classification and regression. Classification involves predicting discrete categories, such as whether an email is spam or not spam. Regression involves predicting continuous values, such as house prices or temperature.\n",
    "\n",
    "Popular supervised learning algorithms include linear regression, logistic regression, decision trees, random forests, support vector machines, and neural networks. Each algorithm has strengths and weaknesses depending on the problem.\n",
    "\n",
    "Section 1.3: Unsupervised Learning\n",
    "\n",
    "Unsupervised learning works with unlabeled data. The algorithm tries to find patterns and structure in the data without being told what to look for. This is useful when you don't know what patterns exist in your data.\n",
    "\n",
    "Common unsupervised learning tasks include clustering and dimensionality reduction. Clustering groups similar data points together, such as customer segmentation. Dimensionality reduction simplifies data while preserving important information.\n",
    "\n",
    "K-means clustering, hierarchical clustering, and DBSCAN are popular clustering algorithms. Principal Component Analysis (PCA) and t-SNE are common dimensionality reduction techniques.\n",
    "\n",
    "Section 1.4: Reinforcement Learning\n",
    "\n",
    "Reinforcement learning is about learning through interaction with an environment. An agent takes actions and receives rewards or penalties. The goal is to learn a policy that maximizes cumulative reward over time.\n",
    "\n",
    "Reinforcement learning has achieved remarkable success in game playing, robotics, and autonomous systems. Famous examples include AlphaGo, which defeated world champions in Go, and self-driving car systems.\n",
    "\n",
    "Key concepts in reinforcement learning include states, actions, rewards, policies, and value functions. Q-learning and policy gradient methods are fundamental algorithms in this field.\n",
    "\n",
    "Chapter 2: Neural Networks and Deep Learning\n",
    "\n",
    "Neural networks are computing systems inspired by biological neural networks. They consist of interconnected nodes (neurons) organized in layers. Deep learning refers to neural networks with many layers.\n",
    "\n",
    "Section 2.1: Neural Network Architecture\n",
    "\n",
    "A basic neural network has three types of layers: input layer, hidden layers, and output layer. The input layer receives data, hidden layers process it, and the output layer produces predictions.\n",
    "\n",
    "Each connection between neurons has a weight that determines the strength of the signal. During training, these weights are adjusted to minimize prediction errors. This process is called backpropagation.\n",
    "\n",
    "Activation functions introduce non-linearity into the network. Common activation functions include ReLU (Rectified Linear Unit), sigmoid, and tanh. Without activation functions, neural networks would only learn linear relationships.\n",
    "\n",
    "Section 2.2: Training Neural Networks\n",
    "\n",
    "Training a neural network involves feeding it data and adjusting weights to minimize a loss function. The loss function measures how far predictions are from actual values. Common loss functions include mean squared error for regression and cross-entropy for classification.\n",
    "\n",
    "Gradient descent is the optimization algorithm used to minimize the loss function. It calculates gradients (derivatives) of the loss with respect to each weight and updates weights in the direction that reduces loss.\n",
    "\n",
    "Learning rate is a crucial hyperparameter that controls how much weights change during each update. Too high a learning rate causes instability; too low makes training very slow. Adaptive learning rate methods like Adam help address this.\n",
    "\n",
    "Section 2.3: Convolutional Neural Networks\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are specialized for processing grid-like data such as images. They use convolutional layers that apply filters to detect features like edges, textures, and patterns.\n",
    "\n",
    "CNNs have revolutionized computer vision tasks including image classification, object detection, and facial recognition. They achieve human-level or better performance on many visual tasks.\n",
    "\n",
    "Key components of CNNs include convolutional layers, pooling layers, and fully connected layers. Convolutional layers detect features, pooling layers reduce dimensionality, and fully connected layers make final predictions.\n",
    "\n",
    "Section 2.4: Recurrent Neural Networks\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are designed for sequential data like text, speech, and time series. Unlike feedforward networks, RNNs have connections that loop back, allowing them to maintain memory of previous inputs.\n",
    "\n",
    "Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs) are advanced RNN architectures that solve the vanishing gradient problem. They can learn long-term dependencies in sequences.\n",
    "\n",
    "RNNs power many natural language processing applications including machine translation, speech recognition, and text generation. However, transformer architectures have recently surpassed RNNs for many NLP tasks.\n",
    "\n",
    "Chapter 3: Model Evaluation and Validation\n",
    "\n",
    "Evaluating machine learning models correctly is crucial for building reliable systems. Poor evaluation can lead to models that perform well in testing but fail in production.\n",
    "\n",
    "Section 3.1: Train-Test Split\n",
    "\n",
    "The most basic evaluation technique is splitting data into training and test sets. The model trains on the training set and is evaluated on the test set. This simulates how the model will perform on new, unseen data.\n",
    "\n",
    "A common split is 80% training and 20% testing, though this varies by dataset size. The key principle is that test data must never be used during training, or evaluation will be overly optimistic.\n",
    "\n",
    "Random splitting works for most cases, but stratified splitting ensures each split has the same proportion of each class. This is important for imbalanced datasets where some classes are rare.\n",
    "\n",
    "Section 3.2: Cross-Validation\n",
    "\n",
    "Cross-validation provides more robust evaluation than a single train-test split. K-fold cross-validation divides data into K subsets (folds). The model trains K times, each time using a different fold as the test set.\n",
    "\n",
    "This gives K performance estimates that can be averaged for a more reliable assessment. Five-fold or ten-fold cross-validation are common choices. Cross-validation is especially valuable with small datasets.\n",
    "\n",
    "Leave-one-out cross-validation is an extreme case where K equals the number of samples. Each sample serves as a test set once. This is computationally expensive but maximizes training data usage.\n",
    "\n",
    "Section 3.3: Evaluation Metrics\n",
    "\n",
    "Different metrics suit different problems. For classification, accuracy measures the proportion of correct predictions. However, accuracy can be misleading with imbalanced classes.\n",
    "\n",
    "Precision measures what proportion of positive predictions are correct. Recall measures what proportion of actual positives are found. The F1 score combines precision and recall into a single metric.\n",
    "\n",
    "For regression, mean squared error (MSE) and mean absolute error (MAE) are common. MSE penalizes large errors more heavily. R-squared measures how much variance the model explains.\n",
    "\n",
    "Section 3.4: Overfitting and Underfitting\n",
    "\n",
    "Overfitting occurs when a model learns training data too well, including noise and outliers. It performs excellently on training data but poorly on new data. Complex models with many parameters are prone to overfitting.\n",
    "\n",
    "Underfitting occurs when a model is too simple to capture patterns in the data. It performs poorly on both training and test data. Finding the right model complexity is a key challenge.\n",
    "\n",
    "Regularization techniques help prevent overfitting by penalizing model complexity. L1 and L2 regularization add penalty terms to the loss function. Dropout randomly deactivates neurons during training in neural networks.\n",
    "\"\"\"\n",
    "    print(f\"‚úÖ Loaded {len(textbook_content)} characters of demo content\")\n",
    "\n",
    "print(f\"\\nüìñ Textbook preview (first 500 characters):\")\n",
    "print(textbook_content[:500] + \"...\")"
   ],
   "id": "2948c5ce839ccef5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 3: Chunking the Textbook\n",
    "\n",
    "We'll split the textbook into chunks of approximately 700 tokens each. This is important because:\n",
    "- LLMs have context limits\n",
    "- Smaller chunks provide more precise retrieval\n",
    "- Each chunk should contain a coherent piece of information\n",
    "\n",
    "We'll use `tiktoken` to count tokens accurately (same tokenizer as GPT models)."
   ],
   "id": "e8734ad028309506"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tiktoken\n",
    "from typing import List, Dict\n",
    "\n",
    "# Initialize tokenizer (using GPT-3.5/GPT-4 encoding)\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Count the number of tokens in a text string.\"\"\"\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def chunk_text(text: str, max_tokens: int = 700, overlap_tokens: int = 50) -> List[Dict[str, any]]:\n",
    "    \"\"\"\n",
    "    Split text into chunks of approximately max_tokens size with overlap.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to chunk\n",
    "        max_tokens: Maximum tokens per chunk\n",
    "        overlap_tokens: Number of tokens to overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with chunk text and metadata\n",
    "    \"\"\"\n",
    "    # Split into paragraphs (preserve document structure)\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "    chunk_id = 0\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        para_tokens = count_tokens(para)\n",
    "        \n",
    "        # If adding this paragraph exceeds max_tokens, save current chunk\n",
    "        if current_tokens + para_tokens > max_tokens and current_chunk:\n",
    "            chunk_text = '\\n\\n'.join(current_chunk)\n",
    "            chunks.append({\n",
    "                'id': chunk_id,\n",
    "                'text': chunk_text,\n",
    "                'token_count': current_tokens,\n",
    "                'char_count': len(chunk_text)\n",
    "            })\n",
    "            chunk_id += 1\n",
    "            \n",
    "            # Start new chunk with overlap (keep last paragraph)\n",
    "            if len(current_chunk) > 1:\n",
    "                current_chunk = [current_chunk[-1]]\n",
    "                current_tokens = count_tokens(current_chunk[0])\n",
    "            else:\n",
    "                current_chunk = []\n",
    "                current_tokens = 0\n",
    "        \n",
    "        current_chunk.append(para)\n",
    "        current_tokens += para_tokens\n",
    "    \n",
    "    # Add the last chunk\n",
    "    if current_chunk:\n",
    "        chunk_text = '\\n\\n'.join(current_chunk)\n",
    "        chunks.append({\n",
    "            'id': chunk_id,\n",
    "            'text': chunk_text,\n",
    "            'token_count': current_tokens,\n",
    "            'char_count': len(chunk_text)\n",
    "        })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Chunk the textbook\n",
    "print(\"‚úÇÔ∏è Chunking textbook into ~700 token pieces...\")\n",
    "chunks = chunk_text(textbook_content, max_tokens=700, overlap_tokens=50)\n",
    "\n",
    "print(f\"\\n‚úÖ Created {len(chunks)} chunks\")\n",
    "print(f\"\\nChunk statistics:\")\n",
    "print(f\"  Average tokens per chunk: {sum(c['token_count'] for c in chunks) / len(chunks):.0f}\")\n",
    "print(f\"  Min tokens: {min(c['token_count'] for c in chunks)}\")\n",
    "print(f\"  Max tokens: {max(c['token_count'] for c in chunks)}\")\n",
    "\n",
    "print(f\"\\nüìÑ Example chunk (Chunk 0):\")\n",
    "print(f\"  Tokens: {chunks[0]['token_count']}\")\n",
    "print(f\"  Characters: {chunks[0]['char_count']}\")\n",
    "print(f\"  Preview: {chunks[0]['text'][:200]}...\")"
   ],
   "id": "680868a7deeff26e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 4: Creating Embeddings and Storing in Qdrant\n",
    "\n",
    "Now we'll:\n",
    "1. Load an embedding model\n",
    "2. Create embeddings for each chunk\n",
    "3. Store them in Qdrant with metadata"
   ],
   "id": "b871a15f57d9b023"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "import numpy as np\n",
    "\n",
    "# Load embedding model\n",
    "print(\"üîÑ Loading embedding model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # 384-dimensional embeddings\n",
    "print(\"‚úÖ Model loaded!\")\n",
    "\n",
    "# Create Qdrant client (in-memory for this demo)\n",
    "print(\"\\nüîÑ Creating Qdrant client...\")\n",
    "client = QdrantClient(\":memory:\")\n",
    "print(\"‚úÖ Qdrant client created (in-memory mode)\")\n",
    "\n",
    "# Create collection for textbook chunks\n",
    "collection_name = \"textbook_chunks\"\n",
    "\n",
    "print(f\"\\nüîÑ Creating collection '{collection_name}'...\")\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(\n",
    "        size=384,  # Dimension of our embeddings\n",
    "        distance=Distance.COSINE  # Cosine similarity\n",
    "    )\n",
    ")\n",
    "print(\"‚úÖ Collection created!\")"
   ],
   "id": "fc73df35c1da67fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create embeddings for all chunks and insert into Qdrant\n",
    "print(\"\\nüîÑ Creating embeddings for all chunks...\")\n",
    "print(\"This may take a minute...\")\n",
    "\n",
    "# Extract just the text from chunks\n",
    "chunk_texts = [chunk['text'] for chunk in chunks]\n",
    "\n",
    "# Create embeddings in batch (faster than one at a time)\n",
    "embeddings = model.encode(chunk_texts, show_progress_bar=True)\n",
    "\n",
    "print(f\"‚úÖ Created {len(embeddings)} embeddings\")\n",
    "print(f\"   Each embedding has {len(embeddings[0])} dimensions\")"
   ],
   "id": "f6947ee1881528cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Prepare points for Qdrant\n",
    "print(\"\\nüîÑ Preparing data for Qdrant...\")\n",
    "\n",
    "points = []\n",
    "for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "    # Extract chapter/section info if available\n",
    "    first_line = chunk['text'].split('\\n')[0]\n",
    "    is_chapter = 'Chapter' in first_line\n",
    "    is_section = 'Section' in first_line\n",
    "\n",
    "    points.append(\n",
    "        PointStruct(\n",
    "            id=i,\n",
    "            vector=embedding.tolist(),\n",
    "            payload={\n",
    "                'text': chunk['text'],\n",
    "                'chunk_id': chunk['id'],\n",
    "                'token_count': chunk['token_count'],\n",
    "                'char_count': chunk['char_count'],\n",
    "                'is_chapter': is_chapter,\n",
    "                'is_section': is_section,\n",
    "                'preview': chunk['text'][:100] + '...'\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Insert into Qdrant\n",
    "client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Inserted {len(points)} chunks into Qdrant!\")\n",
    "print(f\"\\nüìä Collection info:\")\n",
    "collection_info = client.get_collection(collection_name)\n",
    "print(f\"   Vectors count: {collection_info.vectors_count}\")\n",
    "print(f\"   Points count: {collection_info.points_count}\")"
   ],
   "id": "9837ee5169cf02a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 5: How RAG Works - Step by Step\n",
    "\n",
    "Now let's see exactly how a RAG system processes a user question!\n",
    "\n",
    "### Step 1: User asks a question (the prompt)"
   ],
   "id": "4d23440296e682c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# User's question\n",
    "user_question = \"What is supervised learning and what are some examples?\"\n",
    "\n",
    "print(\"‚ùì User Question:\")\n",
    "print(f\"   '{user_question}'\")\n",
    "print(f\"\\nüìä Question stats:\")\n",
    "print(f\"   Tokens: {count_tokens(user_question)}\")\n",
    "print(f\"   Characters: {len(user_question)}\")"
   ],
   "id": "fa70fda4896255b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 2: Convert the question to an embedding\n",
    "\n",
    "The question is embedded using the SAME model that embedded the textbook chunks.\n",
    "This is crucial - both must use the same embedding space!"
   ],
   "id": "8dd01a4d55a858b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"üîÑ Converting question to embedding...\")\n",
    "\n",
    "# Embed the question\n",
    "question_embedding = model.encode([user_question])[0]\n",
    "\n",
    "print(f\"‚úÖ Question embedded!\")\n",
    "print(f\"   Embedding dimensions: {len(question_embedding)}\")\n",
    "print(f\"   First 10 values: {question_embedding[:10]}\")\n",
    "print(f\"   Last 10 values: {question_embedding[-10:]}\")\n",
    "print(f\"\\nüí° This embedding represents the semantic meaning of the question!\")"
   ],
   "id": "807f0beb72eeaabf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 3: Search Qdrant for similar chunks\n",
    "\n",
    "Qdrant compares the question embedding to all chunk embeddings using cosine similarity."
   ],
   "id": "b21a0b67975a3e2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"üîç Searching for relevant chunks in Qdrant...\")\n",
    "\n",
    "# Search for top 3 most relevant chunks\n",
    "search_results = client.search(\n",
    "    collection_name=collection_name,\n",
    "    query_vector=question_embedding.tolist(),\n",
    "    limit=3  # Retrieve top 3 most relevant chunks\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Found {len(search_results)} relevant chunks!\\n\")\n",
    "\n",
    "# Display results with similarity scores\n",
    "for i, result in enumerate(search_results, 1):\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Result #{i} - Similarity Score: {result.score:.4f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Chunk ID: {result.payload['chunk_id']}\")\n",
    "    print(f\"Tokens: {result.payload['token_count']}\")\n",
    "    print(f\"\\nContent Preview:\")\n",
    "    print(result.payload['text'][:300] + \"...\")\n",
    "    print()"
   ],
   "id": "ce71f1cb8abbaa7e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ü§î Understanding Similarity Scores\n",
    "\n",
    "**Similarity scores range from 0 to 1:**\n",
    "- **1.0** = Perfect match (identical meaning)\n",
    "- **0.8-1.0** = Very similar (highly relevant)\n",
    "- **0.6-0.8** = Somewhat similar (possibly relevant)\n",
    "- **< 0.6** = Not very similar (likely not relevant)\n",
    "\n",
    "Notice how the top results have high scores - they're semantically related to the question!"
   ],
   "id": "bafe904c99e8e4f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 4: Format context for the LLM\n",
    "\n",
    "Now we combine the retrieved chunks into a context string that will be sent to an LLM."
   ],
   "id": "7a31cef35450b671"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def format_context(search_results, max_chunks: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Format search results into a context string for the LLM.\n",
    "\n",
    "    Args:\n",
    "        search_results: Results from Qdrant search\n",
    "        max_chunks: Maximum number of chunks to include\n",
    "\n",
    "    Returns:\n",
    "        Formatted context string\n",
    "    \"\"\"\n",
    "    context_parts = []\n",
    "\n",
    "    for i, result in enumerate(search_results[:max_chunks], 1):\n",
    "        context_parts.append(f\"[Source {i} - Relevance: {result.score:.2f}]\")\n",
    "        context_parts.append(result.payload['text'])\n",
    "        context_parts.append(\"\")  # Empty line between sources\n",
    "\n",
    "    return \"\\n\".join(context_parts)\n",
    "\n",
    "# Format the context\n",
    "context = format_context(search_results, max_chunks=3)\n",
    "\n",
    "print(\"üìù Formatted Context for LLM:\")\n",
    "print(\"=\"*80)\n",
    "print(context)\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìä Context stats:\")\n",
    "print(f\"   Total tokens: {count_tokens(context)}\")\n",
    "print(f\"   Total characters: {len(context)}\")"
   ],
   "id": "52169863d03bdd98"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 5: Build the final prompt for the LLM\n",
    "\n",
    "This is what actually gets sent to the LLM (GPT-4, Claude, etc.)"
   ],
   "id": "1494b7ffd9262913"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def build_llm_prompt(user_question: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Build the final prompt that will be sent to the LLM.\n",
    "\n",
    "    Args:\n",
    "        user_question: The user's original question\n",
    "        context: Retrieved context from vector database\n",
    "\n",
    "    Returns:\n",
    "        Complete prompt for the LLM\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a helpful teaching assistant. Answer the student's question using ONLY the information provided in the context below. If the context doesn't contain enough information to answer the question, say so.\n",
    "\n",
    "Context from textbook:\n",
    "{context}\n",
    "\n",
    "Student's question: {user_question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Build the complete prompt\n",
    "final_prompt = build_llm_prompt(user_question, context)\n",
    "\n",
    "print(\"ü§ñ Complete Prompt for LLM:\")\n",
    "print(\"=\"*80)\n",
    "print(final_prompt)\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìä Final prompt stats:\")\n",
    "print(f\"   Total tokens: {count_tokens(final_prompt)}\")\n",
    "print(f\"   Total characters: {len(final_prompt)}\")"
   ],
   "id": "a66edb7e68f85e13"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 6: Send to LLM (Conceptual)\n",
    "\n",
    "In a real application, you would now send this prompt to an LLM API:\n",
    "\n",
    "```python\n",
    "# Example with OpenAI (not executed in this notebook)\n",
    "import openai\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful teaching assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": final_prompt}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(answer)\n",
    "```\n",
    "\n",
    "**For this demo, here's what the LLM would likely respond:**"
   ],
   "id": "a3ac3bb3ba201a57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"ü§ñ Simulated LLM Response:\")\n",
    "print(\"=\"*80)\n",
    "simulated_response = \"\"\"Based on the textbook context provided:\n",
    "\n",
    "Supervised learning is the most common type of machine learning where the algorithm learns from labeled training data. Each training example consists of an input and the desired output, and the algorithm learns to map inputs to outputs.\n",
    "\n",
    "Common examples of supervised learning include:\n",
    "\n",
    "1. **Classification tasks**: Predicting discrete categories, such as:\n",
    "   - Determining whether an email is spam or not spam\n",
    "\n",
    "2. **Regression tasks**: Predicting continuous values, such as:\n",
    "   - House prices\n",
    "   - Temperature\n",
    "\n",
    "Popular supervised learning algorithms mentioned in the textbook include:\n",
    "- Linear regression\n",
    "- Logistic regression\n",
    "- Decision trees\n",
    "- Random forests\n",
    "- Support vector machines\n",
    "- Neural networks\n",
    "\n",
    "Each algorithm has its own strengths and weaknesses depending on the specific problem you're trying to solve.\"\"\"\n",
    "\n",
    "print(simulated_response)\n",
    "print(\"=\"*80)"
   ],
   "id": "a3023f6388a9bb6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 6: Interactive RAG Function\n",
    "\n",
    "Let's create a complete RAG function you can use with any question!"
   ],
   "id": "53e34466bad2cfbe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def rag_query(question: str, top_k: int = 3, show_sources: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline: question -> retrieval -> context formatting.\n",
    "\n",
    "    Args:\n",
    "        question: User's question\n",
    "        top_k: Number of chunks to retrieve\n",
    "        show_sources: Whether to display source information\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with question, context, prompt, and metadata\n",
    "    \"\"\"\n",
    "    # Step 1: Embed the question\n",
    "    question_emb = model.encode([question])[0]\n",
    "\n",
    "    # Step 2: Search Qdrant\n",
    "    results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=question_emb.tolist(),\n",
    "        limit=top_k\n",
    "    )\n",
    "\n",
    "    # Step 3: Format context\n",
    "    context = format_context(results, max_chunks=top_k)\n",
    "\n",
    "    # Step 4: Build final prompt\n",
    "    prompt = build_llm_prompt(question, context)\n",
    "\n",
    "    # Prepare response\n",
    "    response = {\n",
    "        'question': question,\n",
    "        'context': context,\n",
    "        'prompt': prompt,\n",
    "        'num_sources': len(results),\n",
    "        'sources': results,\n",
    "        'total_tokens': count_tokens(prompt)\n",
    "    }\n",
    "\n",
    "    if show_sources:\n",
    "        print(f\"‚ùì Question: {question}\\n\")\n",
    "        print(f\"üìä Retrieved {len(results)} relevant chunks:\\n\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"  {i}. [Score: {result.score:.3f}] {result.payload['preview']}\")\n",
    "        print(f\"\\nüìù Total context tokens: {count_tokens(context)}\")\n",
    "        print(f\"üìù Total prompt tokens: {response['total_tokens']}\")\n",
    "\n",
    "    return response\n",
    "\n",
    "# Test the RAG function\n",
    "print(\"=\"*80)\n",
    "print(\"Testing RAG Query Function\")\n",
    "print(\"=\"*80)\n",
    "result = rag_query(\"What is overfitting and how can we prevent it?\", top_k=3)"
   ],
   "id": "5eabffd6f37eed7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 7: Comparing Different Questions\n",
    "\n",
    "Let's see how the RAG system handles different types of questions!"
   ],
   "id": "86b08231e832dad7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test with different questions\n",
    "test_questions = [\n",
    "    \"What are the three main types of machine learning?\",\n",
    "    \"How do convolutional neural networks work?\",\n",
    "    \"What is the difference between precision and recall?\",\n",
    "    \"Explain gradient descent in simple terms\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing RAG with Multiple Questions\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question {i}/{len(test_questions)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    result = rag_query(question, top_k=2, show_sources=True)\n",
    "    print()"
   ],
   "id": "50da593a9ca09070"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ü§î Observations\n",
    "\n",
    "Look at the results above and consider:\n",
    "1. **How do similarity scores vary?** Some questions have higher scores than others\n",
    "2. **Are the retrieved chunks relevant?** Do they actually contain information to answer the question?\n",
    "3. **What happens with questions not in the textbook?** Try asking about topics not covered!"
   ],
   "id": "97b275332fc59a37"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 8: Advanced RAG Techniques\n",
    "\n",
    "Let's explore some advanced features you can add to your RAG system."
   ],
   "id": "e27902a89ee516f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Technique 1: Filtered Search\n",
    "\n",
    "Search only within specific sections or chapters."
   ],
   "id": "fcb4eeb6ce479a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "\n",
    "# Search only in chapter-level content\n",
    "question = \"What is machine learning?\"\n",
    "question_emb = model.encode([question])[0]\n",
    "\n",
    "filtered_results = client.search(\n",
    "    collection_name=collection_name,\n",
    "    query_vector=question_emb.tolist(),\n",
    "    query_filter=Filter(\n",
    "        must=[\n",
    "            FieldCondition(\n",
    "                key=\"is_chapter\",\n",
    "                match=MatchValue(value=True)\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    limit=3\n",
    ")\n",
    "\n",
    "print(\"üîç Filtered Search (Chapters only):\\n\")\n",
    "for i, result in enumerate(filtered_results, 1):\n",
    "    print(f\"{i}. [Score: {result.score:.3f}]\")\n",
    "    print(f\"   Is Chapter: {result.payload['is_chapter']}\")\n",
    "    print(f\"   Preview: {result.payload['preview']}\\n\")"
   ],
   "id": "fb28934990b97eb6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Technique 2: Hybrid Search with Token Limits\n",
    "\n",
    "Retrieve chunks but ensure total context doesn't exceed token budget."
   ],
   "id": "e7550f1cfc416101"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def rag_query_with_token_limit(question: str, max_context_tokens: int = 1500) -> Dict:\n",
    "    \"\"\"\n",
    "    RAG query that respects a maximum token budget for context.\n",
    "\n",
    "    Args:\n",
    "        question: User's question\n",
    "        max_context_tokens: Maximum tokens allowed in context\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with query results\n",
    "    \"\"\"\n",
    "    # Embed and search\n",
    "    question_emb = model.encode([question])[0]\n",
    "    results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=question_emb.tolist(),\n",
    "        limit=10  # Get more candidates\n",
    "    )\n",
    "\n",
    "    # Add chunks until we hit token limit\n",
    "    selected_chunks = []\n",
    "    total_tokens = 0\n",
    "\n",
    "    for result in results:\n",
    "        chunk_tokens = result.payload['token_count']\n",
    "        if total_tokens + chunk_tokens <= max_context_tokens:\n",
    "            selected_chunks.append(result)\n",
    "            total_tokens += chunk_tokens\n",
    "        else:\n",
    "            break  # Stop when we'd exceed limit\n",
    "\n",
    "    print(f\"üìä Token Budget Management:\")\n",
    "    print(f\"   Max allowed: {max_context_tokens} tokens\")\n",
    "    print(f\"   Actually used: {total_tokens} tokens\")\n",
    "    print(f\"   Chunks included: {len(selected_chunks)}\")\n",
    "    print(f\"   Chunks excluded: {len(results) - len(selected_chunks)}\")\n",
    "\n",
    "    return {\n",
    "        'chunks': selected_chunks,\n",
    "        'total_tokens': total_tokens,\n",
    "        'chunks_used': len(selected_chunks)\n",
    "    }\n",
    "\n",
    "# Test token-limited retrieval\n",
    "result = rag_query_with_token_limit(\n",
    "    \"Explain neural networks and how they are trained\",\n",
    "    max_context_tokens=1000\n",
    ")"
   ],
   "id": "a59d7969ec8e015"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Technique 3: Re-ranking Results\n",
    "\n",
    "Sometimes the initial similarity scores aren't perfect. Re-ranking can improve results."
   ],
   "id": "8b4a0a2bcc62354d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def rerank_by_keyword_presence(results, keywords: List[str]) -> List:\n",
    "    \"\"\"\n",
    "    Re-rank results by boosting chunks that contain specific keywords.\n",
    "\n",
    "    Args:\n",
    "        results: Initial search results from Qdrant\n",
    "        keywords: List of keywords to boost\n",
    "\n",
    "    Returns:\n",
    "        Re-ranked results\n",
    "    \"\"\"\n",
    "    scored_results = []\n",
    "\n",
    "    for result in results:\n",
    "        text_lower = result.payload['text'].lower()\n",
    "\n",
    "        # Count keyword matches\n",
    "        keyword_score = sum(1 for kw in keywords if kw.lower() in text_lower)\n",
    "\n",
    "        # Combine original similarity with keyword boost\n",
    "        combined_score = result.score + (keyword_score * 0.1)  # Boost by 0.1 per keyword\n",
    "\n",
    "        scored_results.append({\n",
    "            'result': result,\n",
    "            'original_score': result.score,\n",
    "            'keyword_matches': keyword_score,\n",
    "            'combined_score': combined_score\n",
    "        })\n",
    "\n",
    "    # Sort by combined score\n",
    "    scored_results.sort(key=lambda x: x['combined_score'], reverse=True)\n",
    "\n",
    "    return scored_results\n",
    "\n",
    "# Example: Search for neural networks and boost results mentioning \"training\"\n",
    "question = \"How are neural networks trained?\"\n",
    "question_emb = model.encode([question])[0]\n",
    "\n",
    "results = client.search(\n",
    "    collection_name=collection_name,\n",
    "    query_vector=question_emb.tolist(),\n",
    "    limit=5\n",
    ")\n",
    "\n",
    "reranked = rerank_by_keyword_presence(results, keywords=[\"training\", \"gradient\", \"backpropagation\"])\n",
    "\n",
    "print(\"üîÑ Re-ranked Results:\\n\")\n",
    "for i, item in enumerate(reranked[:3], 1):\n",
    "    print(f\"{i}. Original Score: {item['original_score']:.3f} | \"\n",
    "          f\"Keyword Matches: {item['keyword_matches']} | \"\n",
    "          f\"Combined Score: {item['combined_score']:.3f}\")\n",
    "    print(f\"   Preview: {item['result'].payload['preview']}\\n\")"
   ],
   "id": "4b0e2fdef815a433"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 9: Student Exercises\n",
    "\n",
    "Now it's your turn! Complete these exercises to build your RAG skills."
   ],
   "id": "98a3ac767cab9eda"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercise 1: Basic RAG Query\n",
    "\n",
    "**Task:** Use the `rag_query()` function to answer: \"What is reinforcement learning?\"\n",
    "\n",
    "**Expected:** You should get chunks from the reinforcement learning section with high similarity scores."
   ],
   "id": "e5deda01e6ac25f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# YOUR CODE HERE\n",
    "# Use rag_query() with the question about reinforcement learning\n",
    "\n"
   ],
   "id": "24558fb4ee8c291"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercise 2: Filtered Search\n",
    "\n",
    "**Task:** Search for information about \"evaluation metrics\" but only in chunks that are sections (not chapters).\n",
    "\n",
    "**Hint:** Use `query_filter` with `is_section=True`"
   ],
   "id": "128b2e3ded0ae545"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Create the question embedding\n",
    "# 2. Use client.search() with a filter for is_section=True\n",
    "# 3. Print the results\n",
    "\n"
   ],
   "id": "ca4ffb47f9927e9f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercise 3: Token Budget\n",
    "\n",
    "**Task:** Create a RAG query for \"What are CNNs and RNNs?\" with a maximum context of 800 tokens.\n",
    "\n",
    "**Hint:** Use the `rag_query_with_token_limit()` function"
   ],
   "id": "7850a02675fc4a5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# YOUR CODE HERE\n",
    "# Use rag_query_with_token_limit() with max_context_tokens=800\n",
    "\n"
   ],
   "id": "d5858d8b0d6d1a17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercise 4: Build a Multi-Question RAG\n",
    "\n",
    "**Task:** Create a function that takes multiple questions and returns combined context for all of them.\n",
    "\n",
    "This is useful when a user asks a complex question that might need information from different parts of the textbook."
   ],
   "id": "e232d4933696f65f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# YOUR CODE HERE\n",
    "def multi_question_rag(questions: List[str], chunks_per_question: int = 2) -> Dict:\n",
    "    \"\"\"\n",
    "    Retrieve context for multiple related questions.\n",
    "\n",
    "    Args:\n",
    "        questions: List of questions to answer\n",
    "        chunks_per_question: How many chunks to retrieve per question\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with combined results\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # 1. For each question, get embeddings and search\n",
    "    # 2. Combine results (remove duplicates)\n",
    "    # 3. Return combined context\n",
    "    pass\n",
    "\n",
    "# Test with related questions\n",
    "# questions = [\n",
    "#     \"What is supervised learning?\",\n",
    "#     \"What is unsupervised learning?\",\n",
    "#     \"What is reinforcement learning?\"\n",
    "# ]\n",
    "# result = multi_question_rag(questions, chunks_per_question=1)"
   ],
   "id": "77359784d4cffed9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercise 5: Evaluation Metrics\n",
    "\n",
    "**Task:** Create a function to evaluate RAG quality by checking if retrieved chunks contain expected keywords.\n",
    "\n",
    "This helps you measure if your RAG system is retrieving relevant information."
   ],
   "id": "b1667e40491390ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# YOUR CODE HERE\n",
    "def evaluate_retrieval(question: str, expected_keywords: List[str], top_k: int = 3) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate retrieval quality by checking for expected keywords.\n",
    "\n",
    "    Args:\n",
    "        question: The question to search for\n",
    "        expected_keywords: Keywords that should appear in good results\n",
    "        top_k: Number of chunks to retrieve\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # 1. Perform RAG query\n",
    "    # 2. Check how many expected keywords appear in results\n",
    "    # 3. Calculate precision: (keywords found / total keywords)\n",
    "    # 4. Return metrics\n",
    "    pass\n",
    "\n",
    "# Test evaluation\n",
    "# metrics = evaluate_retrieval(\n",
    "#     question=\"What is overfitting?\",\n",
    "#     expected_keywords=[\"overfitting\", \"training\", \"test\", \"regularization\"],\n",
    "#     top_k=3\n",
    "# )\n",
    "# print(f\"Keyword coverage: {metrics['precision']:.2%}\")"
   ],
   "id": "7812e4bfcb691f97"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 10: Real-World RAG Considerations\n",
    "\n",
    "### Important Factors for Production RAG Systems\n",
    "\n",
    "**1. Chunk Size Selection**\n",
    "- Too small: Lacks context, may miss important information\n",
    "- Too large: Less precise retrieval, wastes tokens\n",
    "- Sweet spot: 500-1000 tokens depending on your use case\n",
    "\n",
    "**2. Overlap Between Chunks**\n",
    "- Prevents information from being split across chunk boundaries\n",
    "- Typical overlap: 10-20% of chunk size\n",
    "- Trade-off: More storage vs. better retrieval\n",
    "\n",
    "**3. Embedding Model Choice**\n",
    "- Larger models (768+ dimensions): Better accuracy, more expensive\n",
    "- Smaller models (384 dimensions): Faster, cheaper, still good\n",
    "- Domain-specific models: Better for specialized content (medical, legal, etc.)\n",
    "\n",
    "**4. Number of Retrieved Chunks (top_k)**\n",
    "- More chunks: Better coverage, but more noise and cost\n",
    "- Fewer chunks: Faster, cheaper, but might miss information\n",
    "- Typical range: 3-5 chunks for most applications\n",
    "\n",
    "**5. Metadata and Filtering**\n",
    "- Add source, date, author, section, etc.\n",
    "- Enables filtered search (e.g., \"only recent documents\")\n",
    "- Improves relevance and user trust\n",
    "\n",
    "**6. Handling No Good Matches**\n",
    "- Set a minimum similarity threshold (e.g., 0.7)\n",
    "- If no results above threshold, tell user \"I don't have information about that\"\n",
    "- Better than hallucinating an answer!\n",
    "\n",
    "**7. Cost Considerations**\n",
    "- Embedding API costs (if using OpenAI, Cohere, etc.)\n",
    "- Vector database storage costs\n",
    "- LLM API costs (proportional to context size)\n",
    "- Balance quality vs. cost by tuning chunk size and top_k"
   ],
   "id": "624fd0851b80606c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Summary: What You've Learned\n",
    "\n",
    "### RAG Pipeline Steps\n",
    "1. **Document Preparation**: Load and chunk documents into manageable pieces\n",
    "2. **Embedding Creation**: Convert chunks to vector embeddings\n",
    "3. **Vector Storage**: Store embeddings in a vector database (Qdrant)\n",
    "4. **Query Processing**: Convert user questions to embeddings\n",
    "5. **Similarity Search**: Find most relevant chunks using vector similarity\n",
    "6. **Context Assembly**: Combine retrieved chunks into context\n",
    "7. **Prompt Construction**: Build final prompt with context + question\n",
    "8. **LLM Generation**: Send to LLM for answer generation\n",
    "\n",
    "### Key Concepts\n",
    "- ‚úÖ **Chunking**: Breaking documents into semantic units\n",
    "- ‚úÖ **Token counting**: Managing context limits\n",
    "- ‚úÖ **Embeddings**: Converting text to semantic vectors\n",
    "- ‚úÖ **Vector similarity**: Finding semantically related content\n",
    "- ‚úÖ **Context formatting**: Preparing information for LLMs\n",
    "- ‚úÖ **Metadata filtering**: Improving retrieval precision\n",
    "- ‚úÖ **Token budgets**: Managing costs and context limits\n",
    "\n",
    "### Advanced Techniques\n",
    "- üîß Filtered search by metadata\n",
    "- üîß Token-limited retrieval\n",
    "- üîß Result re-ranking\n",
    "- üîß Multi-question queries\n",
    "- üîß Retrieval evaluation\n",
    "\n",
    "**Congratulations!** You now understand how to build a complete RAG system! üéâ\n",
    "\n",
    "### Next Steps\n",
    "1. Try with your own documents\n",
    "2. Experiment with different chunk sizes\n",
    "3. Test different embedding models\n",
    "4. Integrate with a real LLM (OpenAI, Azure OpenAI, etc.)\n",
    "5. Add a user interface (Streamlit, Gradio, etc.)\n",
    "6. Deploy to production!"
   ],
   "id": "5067a3b9856271c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Bonus: Quick Reference Code\n",
    "\n",
    "Here's a complete minimal RAG implementation you can copy and adapt:"
   ],
   "id": "86be576969957738"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "MINIMAL RAG IMPLEMENTATION - QUICK REFERENCE\n",
    "\n",
    "This is a simplified version you can use as a starting point.\n",
    "\"\"\"\n",
    "\n",
    "def minimal_rag(question: str, collection_name: str, client, model, top_k: int = 3):\n",
    "    \"\"\"Minimal RAG implementation in one function.\"\"\"\n",
    "\n",
    "    # 1. Embed question\n",
    "    q_emb = model.encode([question])[0]\n",
    "\n",
    "    # 2. Search vector DB\n",
    "    results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=q_emb.tolist(),\n",
    "        limit=top_k\n",
    "    )\n",
    "\n",
    "    # 3. Build context\n",
    "    context = \"\\n\\n\".join([r.payload['text'] for r in results])\n",
    "\n",
    "    # 4. Build prompt\n",
    "    prompt = f\"\"\"Answer this question using the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Example usage:\n",
    "# prompt = minimal_rag(\"What is machine learning?\", collection_name, client, model)\n",
    "# # Send prompt to your LLM of choice\n",
    "print(\"‚úÖ Minimal RAG function defined!\")\n",
    "print(\"\\nYou can now use minimal_rag() for quick RAG queries!\")"
   ],
   "id": "459fd2d3e0e346f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Resources and Further Reading\n",
    "\n",
    "### Documentation\n",
    "- **Qdrant**: https://qdrant.tech/documentation/\n",
    "- **Sentence Transformers**: https://www.sbert.net/\n",
    "- **Azure Blob Storage**: https://docs.microsoft.com/azure/storage/blobs/\n",
    "\n",
    "### Advanced Topics\n",
    "- **Hybrid Search**: Combining vector search with keyword search\n",
    "- **Re-ranking Models**: Using cross-encoders for better results\n",
    "- **Streaming Responses**: Returning LLM responses in real-time\n",
    "- **Caching**: Storing common queries to reduce costs\n",
    "- **Evaluation**: Measuring RAG system quality\n",
    "\n",
    "### Related Notebooks\n",
    "- Part 1: Why Vector Databases\n",
    "- Part 2: Vector Database Solutions\n",
    "- Part 3: Qdrant & Advanced Features\n",
    "\n",
    "**Happy building!** üöÄ\n",
    "\n",
    "\n"
   ],
   "id": "d68a82c529a15ce7"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
